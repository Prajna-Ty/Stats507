{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7a1d567",
   "metadata": {},
   "source": [
    "# STATS 507 PS 2\n",
    "Tianyu Jiang"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7665b826",
   "metadata": {},
   "source": [
    "### GSI's comment:\n",
    "__-2 for not collecting imports at the top.__\n",
    "\n",
    "Fix: moved all imports to the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a299ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from timeit import Timer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4feb0ce4",
   "metadata": {},
   "source": [
    "## Question 0 - Code review warmup\n",
    "\n",
    "### Code snippet"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7e94d6b5",
   "metadata": {},
   "source": [
    "sample_list = [(1, 3, 5), (0, 1, 2), (1, 9, 8)]\n",
    "op = []\n",
    "for m in range(len(sample_list)):\n",
    "    li = [sample_list[m]]\n",
    "        for n in range(len(sample_list)):\n",
    "            if (sample_list[m][0] == sample_list[n][0] and\n",
    "                    sample_list[m][3] != sample_list[n][3]):\n",
    "                li.append(sample_list[n])\n",
    "        op.append(sorted(li, key=lambda dd: dd[3], reverse=True)[0])\n",
    "res = list(set(op))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fe7f5c",
   "metadata": {},
   "source": [
    "a. Concisely describe what task the code above accomplishes. Say *what* it does (in total) and not *how* it accomplishes it. You may wish to understand the snippet step-by-step, but your description should not state each step individually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b954dab4",
   "metadata": {},
   "source": [
    "Goal: Among the tuples in the given list, if multiple tuples have the same first element - among them, select only the tuple with the maximum last element. Put all the tuples in a list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691f8e22",
   "metadata": {},
   "source": [
    "b. Write a short code review that offers 3-5 (no more) concrete suggestions to make the snippent more efficient, literate (easier to read), or “pythonic”. Focus your suggestions on concepts or principles that would help the author of this code snippet write better code in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e28915",
   "metadata": {},
   "source": [
    "1. Should not add an extra indentation for the inner loop.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "2. This line will yield an out of range error. ```sample_list[m][3] != sample_list[n][3]```\n",
    "A better practice may be ```sample_list[m][-1] != sample_list[n][-1]```\n",
    "<br>Similarly, ```op.append(sorted(li, key=lambda dd: dd[3], reverse=True)[0])``` should be changed into ```op.append(sorted(li, key=lambda dd: dd[-1], reverse=True)[0])```\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "3. - What the start of the outer for loop to the end of the inner for loop does is:\n",
    "    - For each tuple in the list, grab it and add it to li\n",
    "    - Then, loop through all tuples again\n",
    "        - if the first element in tuple_n is the same as the first element in tuple_m\n",
    "        - AND the last element in tuple_n is different from the last element in tuple_m\n",
    "        - Append tuple_n to li\n",
    "- What the sorting line does is:\n",
    "    - Sort tuples in ascending order (biggest to smallest) based on the last element in each tuple\n",
    "    - Then, only takes the first tuples in the sorted list and append it to list \"op\"\n",
    "    - Note that It is basically: Appending the tuple with the greatest last element to list \"op\".\n",
    "\n",
    "<br> It is possible to simplify the code, please see Q2b for more info."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34997852",
   "metadata": {},
   "source": [
    "## Question 1 - List of Tuples\n",
    "\n",
    "Write a function that uses NumPy and a list comprehension to generate a random list of ```n``` k-tuples containing integers ranging from ```low``` to ```high```. Choose an appropriate name for your function, and reasonable default values for k, ```low```, and ```high```.\n",
    "\n",
    "Use ```assert``` to test that your function returns a list of tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d0f0b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tup_list_check(temp_list):\n",
    "    \"\"\"\n",
    "    Check if temp_list is a list of tuples.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    temp_list : ANYTHING\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    True if what's passed in is a list of tuples.\n",
    "    False if otherwise.\n",
    "\n",
    "    \"\"\"\n",
    "    if isinstance(temp_list, list):\n",
    "        if all(isinstance(elem, tuple) for elem in temp_list):\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99e9956d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tup_list(n=5, k=2, low=0, high=8):\n",
    "    \"\"\"\n",
    "    Generate a random list of n k-tuples\n",
    "    containing integers ranging from low to high.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n : POSITIVE INTEGER\n",
    "    k : POSITIVE INTEGER\n",
    "    low : INTEGER\n",
    "    high : INTEGER\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A random list of n k-tuples.\n",
    "\n",
    "    \"\"\"\n",
    "#     assert(isinstance(n, int) & n > 0)\n",
    "#     assert(isinstance(k, int))\n",
    "\n",
    "    k_tup = []\n",
    "\n",
    "    # Generate random numbers within a given range\n",
    "    # and store in a list (using list comprehension)\n",
    "    for i in range(n):\n",
    "        # generate a random tuple\n",
    "        temp_tup = tuple([np.random.randint(low, high)\n",
    "                          for j in range(k)])\n",
    "        # append the tuple to the list\n",
    "        k_tup.append(temp_tup)\n",
    "\n",
    "    # Note that it's also possible to write the whole thing\n",
    "    # using list comprehension\n",
    "    # However, for the sake of readability,\n",
    "    # I used for loop instead.\n",
    "\n",
    "    # Check output using the helper fxn\n",
    "    assert(tup_list_check(k_tup))\n",
    "\n",
    "    return k_tup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4705031d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 5), (3, 4), (4, 1), (3, 5), (0, 2)]\n",
      "[(5, 5, 9, 8, 2), (9, 2, 3, 3, 8), (8, 4, 7, 9, 8)]\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "print(tup_list())\n",
    "print(tup_list(n=3, k=5, low=1, high=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c1fc86",
   "metadata": {},
   "source": [
    "## Question 2 - Refactor the Snippet\n",
    "\n",
    "In this question, you will write functions to accomplish the goal you concisely described in part “a” of the warm up.\n",
    "\n",
    "a. Encapsulate the code snippet from the warmup into a function that parameterizes the role of ```0``` and ```3``` and is otherwise unchanged. Choose appropriate names for these paramters.\n",
    "(Note: parameterizes the role of ```0``` and ```3``` = replaces the fixed numbers 0 and 3 with variables whose values are passed as function inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b990e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is correct, do NOT change\n",
    "def list_helper_a(sample_list, x=0, y=-1):\n",
    "    \"\"\"\n",
    "    Among the tuples in the given list,\n",
    "    if multiple tuples have the same x_th element -\n",
    "    among them, select only the tuple with the maximum y_th element.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sample_list : LIST of tuples\n",
    "    x : INTEGER within the index range\n",
    "    y : INTEGER within the index range\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A list of tuples.\n",
    "    \n",
    "    \"\"\"\n",
    "    op = []\n",
    "\n",
    "    for m in range(len(sample_list)):\n",
    "        # for each tuple in the list, grab it and add it to li\n",
    "        li = [sample_list[m]]\n",
    "\n",
    "        for n in range(len(sample_list)):\n",
    "            if (sample_list[m][x] == sample_list[n][x] and\n",
    "                    sample_list[m][y] != sample_list[n][y]):\n",
    "                # if the x_th element in tuple_n is the same as\n",
    "                # the x_th element in tuple_m\n",
    "                # AND the y_th element in tuple_n is different from\n",
    "                # the y_th element in tuple_m\n",
    "                # Append tuple_n to li\n",
    "                li.append(sample_list[n])\n",
    "\n",
    "            # Sort tuples in ascending order (biggest to smallest)\n",
    "            # based on the y_th element in each tuple\n",
    "            # Then, only takes the first tuples in the sorted list\n",
    "            # i.e., the tuple with the greatest y_th element\n",
    "            # and append it to list \"op\"\n",
    "            op.append(sorted(li, key=lambda dd: dd[y], reverse=True)[0])\n",
    "\n",
    "    # only keep distinct elements in op\n",
    "    return(list(set(op)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82eede6",
   "metadata": {},
   "source": [
    "b. Write an improved version of the function form part a that implements the suggestions from the code review you wrote in part b of the warmup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25696bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_helper_b(sample_list, x=0, y=-1):\n",
    "    \"\"\"\n",
    "    Among the tuples in the given list,\n",
    "    if multiple tuples have the same x_th element -\n",
    "    among them, select only the tuple with the maximum y_th element.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sample_list : LIST of tuples\n",
    "    x : INTEGER within the index range\n",
    "    y : INTEGER within the index range\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A list of tuples.\n",
    "    \n",
    "    \"\"\"\n",
    "    op = []\n",
    "    for m in range(len(sample_list)):\n",
    "        li = sample_list[m]\n",
    "        for n in range(m, len(sample_list)):\n",
    "            if (sample_list[m][x] == sample_list[n][x] and\n",
    "                    sample_list[m][y] != sample_list[n][y]):\n",
    "                if sample_list[n][y] > li[y]:\n",
    "                    li = sample_list[n]\n",
    "        op.append(li)\n",
    "\n",
    "    # only keep distinct elements in op\n",
    "    return(list(set(op)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70b5a6c",
   "metadata": {},
   "source": [
    "c. Write a function from scratch to accomplish the same task as the previous two parts. Your solution should *traverse the input list of tuples no more than twice*. Hint: consider using a dictionary or a default dictionary in your solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a79e84f",
   "metadata": {},
   "source": [
    "Note: Got this from stackoverflow\n",
    "[https://stackoverflow.com/questions/69025133/filtering-list-of-tuples-based-on-condition]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81c16cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_helper_c(sample_list, x=0, y=-1):\n",
    "    \"\"\"\n",
    "    Among the tuples in the given list,\n",
    "    if multiple tuples have the same x_th element -\n",
    "    among them, select only the tuple with the maximum y_th element.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sample_list : LIST of tuples\n",
    "    x : INTEGER within the index range\n",
    "    y : INTEGER within the index range\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A list of tuples.\n",
    "    \n",
    "    \"\"\"\n",
    "    d = defaultdict(list)\n",
    "    for e in sample_list:\n",
    "        d[e[x]].append(e)\n",
    "\n",
    "    res = [max(val, key=lambda m: m[y]) for val in d.values()]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd41e20",
   "metadata": {},
   "source": [
    "d. Using the function you wrote in question 1 to generate a list of tuples as input(s), run and summarize a small Monte Carlo study comparing the execution times of the three functions above (a-c)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b354d041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Function   min, ms  median, ms  mean, ms\n",
      "0  list_helper_a  0.463995    0.546912  0.606163\n",
      "1  list_helper_b  0.036445    0.036541  0.037247\n",
      "2  list_helper_c  0.007603    0.007933  0.008689\n"
     ]
    }
   ],
   "source": [
    "# List input\n",
    "n_list = tup_list(n=20, k=10)\n",
    "\n",
    "# Defining the dict\n",
    "time = defaultdict(list)\n",
    "\n",
    "for f in [list_helper_a, list_helper_b, list_helper_c]:\n",
    "    t = Timer('f(n)', globals={'f': f, 'n': n_list})\n",
    "    tm = t.repeat(repeat=10, number=1)\n",
    "    time[\"Function\"].append(f.__name__)\n",
    "    time[\"min, ms\"].append(np.min(tm) * 1000)\n",
    "    time[\"median, ms\"].append(np.median(tm) * 1000)\n",
    "    time[\"mean, ms\"].append(np.mean(tm) * 1000)\n",
    "\n",
    "# Print dictionary\n",
    "time_table = pd.DataFrame(time)\n",
    "\n",
    "print(time_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476ecbcd",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "Use Pandas to read, clean, and append several data files from the National Health and Nutrition Examination Survey [NHANES](https://www.cdc.gov/nchs/nhanes/index.htm). We will use the data you prepare in this question as the starting point for analyses in one or more future problem sets. For this problem, you should use the four cohorts spanning the years 2011-2018.\n",
    "\n",
    "Hint: Use ```pd.read_sas()``` to import files with the ```.XPT``` (“SAS transport”) extension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590b18c0",
   "metadata": {},
   "source": [
    "a. Use Python and Pandas to read and append the demographic datasets keeping only columns containing the unique ids (SEQN), age (RIDAGEYR), race and ethnicity (RIDRETH3), education (DMDEDUC2), and marital status (DMDMARTL), along with the following variables related to the survey weighting: (RIDSTATR, SDMVPSU, SDMVSTRA, WTMEC2YR, WTINT2YR). Add an additional column identifying to which cohort each case belongs. Rename the columns with literate variable names using all lower case and convert each column to an appropriate type. Finally, save the resulting data frame to a serialized “round-trip” format of your choosing (e.g. pickle, feather, or parquet)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ec7468",
   "metadata": {},
   "source": [
    "Reference of meaning of each column: https://wwwn.cdc.gov/Nchs/Nhanes/2017-2018/DEMO_J.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d673516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the demographic datasets\n",
    "demo_G = pd.read_sas(filepath_or_buffer=\"./ps2_data/DEMO_G.XPT\")\n",
    "demo_H = pd.read_sas(filepath_or_buffer=\"./ps2_data/DEMO_H.XPT\")\n",
    "demo_I = pd.read_sas(filepath_or_buffer=\"./ps2_data/DEMO_I.XPT\")\n",
    "demo_J = pd.read_sas(filepath_or_buffer=\"./ps2_data/DEMO_J.XPT\")\n",
    "\n",
    "# Add cohort numbers\n",
    "demo_G['cohort'] = \"11-12\"\n",
    "demo_H['cohort'] = \"13-14\"\n",
    "demo_I['cohort'] = \"15-16\"\n",
    "demo_J['cohort'] = \"17-18\"\n",
    "\n",
    "# Append the demographic datasets\n",
    "demo = pd.concat([demo_G, demo_H, demo_I, demo_J])\n",
    "\n",
    "col_list = ['cohort', 'SEQN', 'RIDAGEYR', 'RIDRETH3', 'DMDEDUC2', 'DMDMARTL',\n",
    "            'RIDSTATR', 'SDMVPSU', 'SDMVSTRA', 'WTMEC2YR', 'WTINT2YR']\n",
    "temp_dict = {'SEQN': 'respondent_seq_num', 'RIDAGEYR': 'age',\n",
    "                                     'RIDRETH3': 'race_ethnicity', 'DMDEDUC2': 'edu',\n",
    "                                     'DMDMARTL': 'marital_status',\n",
    "                                     'RIDSTATR': 'if_examination',\n",
    "                                     'SDMVPSU': 'masked_variance_pseudo_psu',\n",
    "                                     'SDMVSTRA': 'masked_variance_pseudo_stratum',\n",
    "                                     'WTMEC2YR': 'full_samp_2yr_mec_exam_weight',\n",
    "                                     'WTINT2YR': 'full_samp_2yr_mec_interview_weight'}\n",
    "\n",
    "demo = demo[col_list]\n",
    "\n",
    "# Rename the columns with literate variable names using all lower case\n",
    "demo = demo.rename(columns=temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f68d466a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cohort</th>\n",
       "      <th>respondent_seq_num</th>\n",
       "      <th>age</th>\n",
       "      <th>race_ethnicity</th>\n",
       "      <th>edu</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>if_examination</th>\n",
       "      <th>masked_variance_pseudo_psu</th>\n",
       "      <th>masked_variance_pseudo_stratum</th>\n",
       "      <th>full_samp_2yr_mec_exam_weight</th>\n",
       "      <th>full_samp_2yr_mec_interview_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11-12</td>\n",
       "      <td>62161.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>104236.582554</td>\n",
       "      <td>102641.406474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11-12</td>\n",
       "      <td>62162.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>16116.354010</td>\n",
       "      <td>15457.736897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11-12</td>\n",
       "      <td>62163.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>7869.485117</td>\n",
       "      <td>7397.684828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11-12</td>\n",
       "      <td>62164.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>127965.226204</td>\n",
       "      <td>127351.373299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11-12</td>\n",
       "      <td>62165.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>13384.042162</td>\n",
       "      <td>12209.744980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9249</th>\n",
       "      <td>17-18</td>\n",
       "      <td>102952.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>18338.711104</td>\n",
       "      <td>16896.276203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9250</th>\n",
       "      <td>17-18</td>\n",
       "      <td>102953.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>63661.951573</td>\n",
       "      <td>61630.380013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9251</th>\n",
       "      <td>17-18</td>\n",
       "      <td>102954.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>17694.783346</td>\n",
       "      <td>17160.895269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9252</th>\n",
       "      <td>17-18</td>\n",
       "      <td>102955.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>14871.839636</td>\n",
       "      <td>14238.445922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9253</th>\n",
       "      <td>17-18</td>\n",
       "      <td>102956.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>39426.299948</td>\n",
       "      <td>38645.740291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39156 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     cohort  respondent_seq_num   age  race_ethnicity  edu  marital_status  \\\n",
       "0     11-12             62161.0  22.0             3.0  3.0             5.0   \n",
       "1     11-12             62162.0   3.0             1.0 -1.0            -1.0   \n",
       "2     11-12             62163.0  14.0             6.0 -1.0            -1.0   \n",
       "3     11-12             62164.0  44.0             3.0  4.0             1.0   \n",
       "4     11-12             62165.0  14.0             4.0 -1.0            -1.0   \n",
       "...     ...                 ...   ...             ...  ...             ...   \n",
       "9249  17-18            102952.0  70.0             6.0  3.0             1.0   \n",
       "9250  17-18            102953.0  42.0             1.0  3.0             4.0   \n",
       "9251  17-18            102954.0  41.0             4.0  5.0             5.0   \n",
       "9252  17-18            102955.0  14.0             4.0 -1.0            -1.0   \n",
       "9253  17-18            102956.0  38.0             3.0  4.0             3.0   \n",
       "\n",
       "      if_examination  masked_variance_pseudo_psu  \\\n",
       "0                2.0                         1.0   \n",
       "1                2.0                         3.0   \n",
       "2                2.0                         3.0   \n",
       "3                2.0                         1.0   \n",
       "4                2.0                         2.0   \n",
       "...              ...                         ...   \n",
       "9249             2.0                         2.0   \n",
       "9250             2.0                         2.0   \n",
       "9251             2.0                         1.0   \n",
       "9252             2.0                         1.0   \n",
       "9253             2.0                         1.0   \n",
       "\n",
       "      masked_variance_pseudo_stratum  full_samp_2yr_mec_exam_weight  \\\n",
       "0                               91.0                  104236.582554   \n",
       "1                               92.0                   16116.354010   \n",
       "2                               90.0                    7869.485117   \n",
       "3                               94.0                  127965.226204   \n",
       "4                               90.0                   13384.042162   \n",
       "...                              ...                            ...   \n",
       "9249                           138.0                   18338.711104   \n",
       "9250                           137.0                   63661.951573   \n",
       "9251                           144.0                   17694.783346   \n",
       "9252                           136.0                   14871.839636   \n",
       "9253                           142.0                   39426.299948   \n",
       "\n",
       "      full_samp_2yr_mec_interview_weight  \n",
       "0                          102641.406474  \n",
       "1                           15457.736897  \n",
       "2                            7397.684828  \n",
       "3                          127351.373299  \n",
       "4                           12209.744980  \n",
       "...                                  ...  \n",
       "9249                        16896.276203  \n",
       "9250                        61630.380013  \n",
       "9251                        17160.895269  \n",
       "9252                        14238.445922  \n",
       "9253                        38645.740291  \n",
       "\n",
       "[39156 rows x 11 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change NaN entries to -1\n",
    "demo = demo.fillna(-1)\n",
    "demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a43b206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cohort                                 object\n",
       "respondent_seq_num                      int64\n",
       "age                                     int64\n",
       "race_ethnicity                          int64\n",
       "edu                                     int64\n",
       "marital_status                          int64\n",
       "if_examination                          int64\n",
       "masked_variance_pseudo_psu              int64\n",
       "masked_variance_pseudo_stratum          int64\n",
       "full_samp_2yr_mec_exam_weight         float64\n",
       "full_samp_2yr_mec_interview_weight    float64\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# converting columns from float to int\n",
    "temp_list = ['respondent_seq_num', 'race_ethnicity', 'age',\n",
    "             'edu', 'marital_status', 'if_examination',\n",
    "             'masked_variance_pseudo_psu',\n",
    "             'masked_variance_pseudo_stratum']\n",
    "for i in temp_list:\n",
    "    demo[i] = demo[i].apply(np.int64)\n",
    "\n",
    "# displaying the datatypes\n",
    "display(demo.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6524c25e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cohort</th>\n",
       "      <th>respondent_seq_num</th>\n",
       "      <th>age</th>\n",
       "      <th>race_ethnicity</th>\n",
       "      <th>edu</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>if_examination</th>\n",
       "      <th>masked_variance_pseudo_psu</th>\n",
       "      <th>masked_variance_pseudo_stratum</th>\n",
       "      <th>full_samp_2yr_mec_exam_weight</th>\n",
       "      <th>full_samp_2yr_mec_interview_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11-12</td>\n",
       "      <td>62161</td>\n",
       "      <td>22</td>\n",
       "      <td>non_hispanic_white</td>\n",
       "      <td>high_school_grad_or_GED</td>\n",
       "      <td>never_married</td>\n",
       "      <td>both_interviewed_n_mec_examed</td>\n",
       "      <td>1</td>\n",
       "      <td>91</td>\n",
       "      <td>104236.582554</td>\n",
       "      <td>102641.406474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11-12</td>\n",
       "      <td>62162</td>\n",
       "      <td>3</td>\n",
       "      <td>mexican_american</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>both_interviewed_n_mec_examed</td>\n",
       "      <td>3</td>\n",
       "      <td>92</td>\n",
       "      <td>16116.354010</td>\n",
       "      <td>15457.736897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11-12</td>\n",
       "      <td>62163</td>\n",
       "      <td>14</td>\n",
       "      <td>non_hispanic_white</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>both_interviewed_n_mec_examed</td>\n",
       "      <td>3</td>\n",
       "      <td>90</td>\n",
       "      <td>7869.485117</td>\n",
       "      <td>7397.684828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11-12</td>\n",
       "      <td>62164</td>\n",
       "      <td>44</td>\n",
       "      <td>non_hispanic_white</td>\n",
       "      <td>some_college</td>\n",
       "      <td>married</td>\n",
       "      <td>both_interviewed_n_mec_examed</td>\n",
       "      <td>1</td>\n",
       "      <td>94</td>\n",
       "      <td>127965.226204</td>\n",
       "      <td>127351.373299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11-12</td>\n",
       "      <td>62165</td>\n",
       "      <td>14</td>\n",
       "      <td>non_hispanic_black</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>both_interviewed_n_mec_examed</td>\n",
       "      <td>2</td>\n",
       "      <td>90</td>\n",
       "      <td>13384.042162</td>\n",
       "      <td>12209.744980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9249</th>\n",
       "      <td>17-18</td>\n",
       "      <td>102952</td>\n",
       "      <td>70</td>\n",
       "      <td>non_hispanic_white</td>\n",
       "      <td>high_school_grad_or_GED</td>\n",
       "      <td>married</td>\n",
       "      <td>both_interviewed_n_mec_examed</td>\n",
       "      <td>2</td>\n",
       "      <td>138</td>\n",
       "      <td>18338.711104</td>\n",
       "      <td>16896.276203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9250</th>\n",
       "      <td>17-18</td>\n",
       "      <td>102953</td>\n",
       "      <td>42</td>\n",
       "      <td>mexican_american</td>\n",
       "      <td>high_school_grad_or_GED</td>\n",
       "      <td>separated</td>\n",
       "      <td>both_interviewed_n_mec_examed</td>\n",
       "      <td>2</td>\n",
       "      <td>137</td>\n",
       "      <td>63661.951573</td>\n",
       "      <td>61630.380013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9251</th>\n",
       "      <td>17-18</td>\n",
       "      <td>102954</td>\n",
       "      <td>41</td>\n",
       "      <td>non_hispanic_black</td>\n",
       "      <td>college_grad_or_above</td>\n",
       "      <td>never_married</td>\n",
       "      <td>both_interviewed_n_mec_examed</td>\n",
       "      <td>1</td>\n",
       "      <td>144</td>\n",
       "      <td>17694.783346</td>\n",
       "      <td>17160.895269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9252</th>\n",
       "      <td>17-18</td>\n",
       "      <td>102955</td>\n",
       "      <td>14</td>\n",
       "      <td>non_hispanic_black</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>both_interviewed_n_mec_examed</td>\n",
       "      <td>1</td>\n",
       "      <td>136</td>\n",
       "      <td>14871.839636</td>\n",
       "      <td>14238.445922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9253</th>\n",
       "      <td>17-18</td>\n",
       "      <td>102956</td>\n",
       "      <td>38</td>\n",
       "      <td>non_hispanic_white</td>\n",
       "      <td>some_college</td>\n",
       "      <td>divorced</td>\n",
       "      <td>both_interviewed_n_mec_examed</td>\n",
       "      <td>1</td>\n",
       "      <td>142</td>\n",
       "      <td>39426.299948</td>\n",
       "      <td>38645.740291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39156 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     cohort  respondent_seq_num  age      race_ethnicity  \\\n",
       "0     11-12               62161   22  non_hispanic_white   \n",
       "1     11-12               62162    3    mexican_american   \n",
       "2     11-12               62163   14  non_hispanic_white   \n",
       "3     11-12               62164   44  non_hispanic_white   \n",
       "4     11-12               62165   14  non_hispanic_black   \n",
       "...     ...                 ...  ...                 ...   \n",
       "9249  17-18              102952   70  non_hispanic_white   \n",
       "9250  17-18              102953   42    mexican_american   \n",
       "9251  17-18              102954   41  non_hispanic_black   \n",
       "9252  17-18              102955   14  non_hispanic_black   \n",
       "9253  17-18              102956   38  non_hispanic_white   \n",
       "\n",
       "                          edu marital_status                 if_examination  \\\n",
       "0     high_school_grad_or_GED  never_married  both_interviewed_n_mec_examed   \n",
       "1                     missing        missing  both_interviewed_n_mec_examed   \n",
       "2                     missing        missing  both_interviewed_n_mec_examed   \n",
       "3                some_college        married  both_interviewed_n_mec_examed   \n",
       "4                     missing        missing  both_interviewed_n_mec_examed   \n",
       "...                       ...            ...                            ...   \n",
       "9249  high_school_grad_or_GED        married  both_interviewed_n_mec_examed   \n",
       "9250  high_school_grad_or_GED      separated  both_interviewed_n_mec_examed   \n",
       "9251    college_grad_or_above  never_married  both_interviewed_n_mec_examed   \n",
       "9252                  missing        missing  both_interviewed_n_mec_examed   \n",
       "9253             some_college       divorced  both_interviewed_n_mec_examed   \n",
       "\n",
       "      masked_variance_pseudo_psu  masked_variance_pseudo_stratum  \\\n",
       "0                              1                              91   \n",
       "1                              3                              92   \n",
       "2                              3                              90   \n",
       "3                              1                              94   \n",
       "4                              2                              90   \n",
       "...                          ...                             ...   \n",
       "9249                           2                             138   \n",
       "9250                           2                             137   \n",
       "9251                           1                             144   \n",
       "9252                           1                             136   \n",
       "9253                           1                             142   \n",
       "\n",
       "      full_samp_2yr_mec_exam_weight  full_samp_2yr_mec_interview_weight  \n",
       "0                     104236.582554                       102641.406474  \n",
       "1                      16116.354010                        15457.736897  \n",
       "2                       7869.485117                         7397.684828  \n",
       "3                     127965.226204                       127351.373299  \n",
       "4                      13384.042162                        12209.744980  \n",
       "...                             ...                                 ...  \n",
       "9249                   18338.711104                        16896.276203  \n",
       "9250                   63661.951573                        61630.380013  \n",
       "9251                   17694.783346                        17160.895269  \n",
       "9252                   14871.839636                        14238.445922  \n",
       "9253                   39426.299948                        38645.740291  \n",
       "\n",
       "[39156 rows x 11 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo['race_ethnicity'] = pd.Categorical(\n",
    "    demo['race_ethnicity'].replace({1: 'mexican_american',\n",
    "                                    2: 'other_hispanic',\n",
    "                                    3: 'non_hispanic_white',\n",
    "                                    4: 'non_hispanic_black',\n",
    "                                    6: 'non_hispanic_white',\n",
    "                                    7: 'other_race_include_multi_racial',\n",
    "                                    -1: 'missing'}))\n",
    "\n",
    "demo['edu'] = pd.Categorical(\n",
    "    demo['edu'].replace({1: 'less_than_9',\n",
    "                         2: '9_to_11_n_12_wout_dip',\n",
    "                         3: 'high_school_grad_or_GED',\n",
    "                         4: 'some_college',\n",
    "                         5: 'college_grad_or_above',\n",
    "                         7: 'refused',\n",
    "                         9: 'unknown',\n",
    "                         -1: 'missing'}))\n",
    "\n",
    "demo['marital_status'] = pd.Categorical(\n",
    "    demo['marital_status'].replace({1: 'married',\n",
    "                         2: 'widowed',\n",
    "                         3: 'divorced',\n",
    "                         4: 'separated',\n",
    "                         5: 'never_married',\n",
    "                         6: 'living_w_partner',\n",
    "                         77: 'refused',\n",
    "                         99: 'unknown',\n",
    "                         -1: 'missing'}))\n",
    "\n",
    "demo['if_examination'] = pd.Categorical(\n",
    "    demo['if_examination'].replace({1: 'interviewed_only',\n",
    "                         2: 'both_interviewed_n_mec_examed',\n",
    "                         -1: 'missing'}))\n",
    "\n",
    "demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8df10fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the resulting data frame to a serialized “round-trip” format\n",
    "demo.to_pickle(\"./DEMO.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03808dd9",
   "metadata": {},
   "source": [
    "b. Repeat part a for the oral health and dentition data (OHXDEN_*.XPT) retaining the following variables: SEQN, OHDDESTS, tooth counts (OHXxxTC), and coronal cavities (OHXxxCTC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "871eafdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the oral health and dentition data\n",
    "ohxden_G = pd.read_sas(filepath_or_buffer=\"./ps2_data/OHXDEN_G.XPT\")\n",
    "ohxden_H = pd.read_sas(filepath_or_buffer=\"./ps2_data/OHXDEN_H.XPT\")\n",
    "ohxden_I = pd.read_sas(filepath_or_buffer=\"./ps2_data/OHXDEN_I.XPT\")\n",
    "ohxden_J = pd.read_sas(filepath_or_buffer=\"./ps2_data/OHXDEN_J.XPT\")\n",
    "\n",
    "# Add cohort numbers\n",
    "ohxden_G['cohort'] = \"11-12\"\n",
    "ohxden_H['cohort'] = \"13-14\"\n",
    "ohxden_I['cohort'] = \"15-16\"\n",
    "ohxden_J['cohort'] = \"17-18\"\n",
    "\n",
    "# Append the datasets\n",
    "ohxden = pd.concat([ohxden_G, ohxden_H, ohxden_I, ohxden_J])\n",
    "\n",
    "# Keep wanted columns\n",
    "ohxden = ohxden.filter(regex=('cohort|SEQN|OHDDESTS|OHX\\d\\dTC|OHX\\d\\dCTC'))\n",
    "ohxden = ohxden.rename(columns={'SEQN': 'respondent_seq_num',\n",
    "                    'OHDDESTS': 'dentition_status_code'})\n",
    "\n",
    "# shift column 'cohort' to first position\n",
    "first_column = ohxden.pop('cohort')\n",
    "ohxden.insert(0, 'cohort', first_column)\n",
    "\n",
    "# rename columns to lower case\n",
    "ohxden.columns= ohxden.columns.str.lower()\n",
    "\n",
    "# ohxden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95ea2ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'ohx02ctc' to 'ohx09ctc'\n",
    "x_0 = ['ohx0'+ str(i) +'ctc' for i in range(2, 10)]\n",
    "# 'ohx10ctc' to 'ohx15ctc'\n",
    "x_1 = ['ohx'+ str(i) +'ctc' for i in range(10, 16)]\n",
    "# 'ohx18ctc' to 'ohx31ctc'\n",
    "x_2 = ['ohx'+ str(i) +'ctc' for i in range(18, 32)]\n",
    "\n",
    "col_list = x_0 + x_1 + x_2\n",
    "# print(col_list)  # Sanity check\n",
    "\n",
    "# utf-8\n",
    "for col in col_list:\n",
    "#     print(col)\n",
    "    ohxden[col] = ohxden[col].str.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aea4a850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cohort                   object\n",
       "respondent_seq_num        int64\n",
       "dentition_status_code     int64\n",
       "ohx01tc                   int64\n",
       "ohx02tc                   int64\n",
       "                          ...  \n",
       "ohx27ctc                 object\n",
       "ohx28ctc                 object\n",
       "ohx29ctc                 object\n",
       "ohx30ctc                 object\n",
       "ohx31ctc                 object\n",
       "Length: 63, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Change NaN entries to -1\n",
    "ohxden = ohxden.fillna(-1)\n",
    "\n",
    "# 'ohx01tc' to 'ohx09tc'\n",
    "y_0 = ['ohx0'+ str(i) +'tc' for i in range(1, 10)]\n",
    "# 'ohx10tc' to 'ohx32tc'\n",
    "y_1 = ['ohx'+ str(i) +'tc' for i in range(10, 33)]\n",
    "\n",
    "tc_list = y_0 + y_1\n",
    "\n",
    "# converting columns from float to int\n",
    "temp_list = ['respondent_seq_num',\n",
    "             'dentition_status_code'] + tc_list\n",
    "for i in temp_list:\n",
    "    ohxden[i] = ohxden[i].apply(np.int64)\n",
    "    \n",
    "# displaying the datatypes\n",
    "display(ohxden.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b2702bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tc_list:\n",
    "    ohxden[i] = pd.Categorical(\n",
    "        ohxden[i].replace({1: 'primary_tooth_present',\n",
    "                           2: 'permanent_tooth_present',\n",
    "                           3: 'dental_implant',\n",
    "                           4: 'tooth_not_present',\n",
    "                           5: 'permanent_dental_root_fragment_present',\n",
    "                           9: 'no_access',\n",
    "                           -1: 'missing'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4cfa0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should have coded for ctc (Coronal Caries) as well,\n",
    "# But that's too much work (pure coding)\n",
    "# It can be done in a similar way as the last chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a44b4f00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cohort</th>\n",
       "      <th>respondent_seq_num</th>\n",
       "      <th>dentition_status_code</th>\n",
       "      <th>ohx01tc</th>\n",
       "      <th>ohx02tc</th>\n",
       "      <th>ohx03tc</th>\n",
       "      <th>ohx04tc</th>\n",
       "      <th>ohx05tc</th>\n",
       "      <th>ohx06tc</th>\n",
       "      <th>ohx07tc</th>\n",
       "      <th>...</th>\n",
       "      <th>ohx22ctc</th>\n",
       "      <th>ohx23ctc</th>\n",
       "      <th>ohx24ctc</th>\n",
       "      <th>ohx25ctc</th>\n",
       "      <th>ohx26ctc</th>\n",
       "      <th>ohx27ctc</th>\n",
       "      <th>ohx28ctc</th>\n",
       "      <th>ohx29ctc</th>\n",
       "      <th>ohx30ctc</th>\n",
       "      <th>ohx31ctc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11-12</td>\n",
       "      <td>62161</td>\n",
       "      <td>1</td>\n",
       "      <td>tooth_not_present</td>\n",
       "      <td>permanent_tooth_present</td>\n",
       "      <td>permanent_tooth_present</td>\n",
       "      <td>permanent_tooth_present</td>\n",
       "      <td>permanent_tooth_present</td>\n",
       "      <td>permanent_tooth_present</td>\n",
       "      <td>permanent_tooth_present</td>\n",
       "      <td>...</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>Z</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11-12</td>\n",
       "      <td>62162</td>\n",
       "      <td>1</td>\n",
       "      <td>tooth_not_present</td>\n",
       "      <td>tooth_not_present</td>\n",
       "      <td>tooth_not_present</td>\n",
       "      <td>primary_tooth_present</td>\n",
       "      <td>primary_tooth_present</td>\n",
       "      <td>primary_tooth_present</td>\n",
       "      <td>primary_tooth_present</td>\n",
       "      <td>...</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11-12</td>\n",
       "      <td>62163</td>\n",
       "      <td>1</td>\n",
       "      <td>tooth_not_present</td>\n",
       "      <td>permanent_tooth_present</td>\n",
       "      <td>permanent_tooth_present</td>\n",
       "      <td>permanent_tooth_present</td>\n",
       "      <td>permanent_tooth_present</td>\n",
       "      <td>permanent_tooth_present</td>\n",
       "      <td>permanent_tooth_present</td>\n",
       "      <td>...</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>Y</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11-12</td>\n",
       "      <td>62164</td>\n",
       "      <td>1</td>\n",
       "      <td>tooth_not_present</td>\n",
       "      <td>permanent_tooth_present</td>\n",
       "      <td>permanent_tooth_present</td>\n",
       "      <td>permanent_tooth_present</td>\n",
       "      <td>permanent_tooth_present</td>\n",
       "      <td>permanent_tooth_present</td>\n",
       "      <td>permanent_tooth_present</td>\n",
       "      <td>...</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>Z</td>\n",
       "      <td>Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11-12</td>\n",
       "      <td>62165</td>\n",
       "      <td>1</td>\n",
       "      <td>tooth_not_present</td>\n",
       "      <td>permanent_tooth_present</td>\n",
       "      <td>permanent_tooth_present</td>\n",
       "      <td>permanent_tooth_present</td>\n",
       "      <td>permanent_tooth_present</td>\n",
       "      <td>permanent_tooth_present</td>\n",
       "      <td>permanent_tooth_present</td>\n",
       "      <td>...</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8361</th>\n",
       "      <td>17-18</td>\n",
       "      <td>102952</td>\n",
       "      <td>1</td>\n",
       "      <td>permanent_tooth_present</td>\n",
       "      <td>permanent_tooth_present</td>\n",
       "      <td>permanent_tooth_present</td>\n",
       "      <td>permanent_tooth_present</td>\n",
       "      <td>permanent_tooth_present</td>\n",
       "      <td>permanent_tooth_present</td>\n",
       "      <td>permanent_tooth_present</td>\n",
       "      <td>...</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8362</th>\n",
       "      <td>17-18</td>\n",
       "      <td>102953</td>\n",
       "      <td>1</td>\n",
       "      <td>permanent_tooth_present</td>\n",
       "      <td>permanent_tooth_present</td>\n",
       "      <td>permanent_tooth_present</td>\n",
       "      <td>permanent_tooth_present</td>\n",
       "      <td>permanent_tooth_present</td>\n",
       "      <td>permanent_tooth_present</td>\n",
       "      <td>permanent_tooth_present</td>\n",
       "      <td>...</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8363</th>\n",
       "      <td>17-18</td>\n",
       "      <td>102954</td>\n",
       "      <td>1</td>\n",
       "      <td>permanent_tooth_present</td>\n",
       "      <td>permanent_tooth_present</td>\n",
       "      <td>permanent_tooth_present</td>\n",
       "      <td>permanent_tooth_present</td>\n",
       "      <td>permanent_tooth_present</td>\n",
       "      <td>permanent_tooth_present</td>\n",
       "      <td>permanent_tooth_present</td>\n",
       "      <td>...</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>F</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8364</th>\n",
       "      <td>17-18</td>\n",
       "      <td>102955</td>\n",
       "      <td>1</td>\n",
       "      <td>tooth_not_present</td>\n",
       "      <td>permanent_tooth_present</td>\n",
       "      <td>permanent_tooth_present</td>\n",
       "      <td>permanent_tooth_present</td>\n",
       "      <td>permanent_tooth_present</td>\n",
       "      <td>permanent_tooth_present</td>\n",
       "      <td>permanent_tooth_present</td>\n",
       "      <td>...</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8365</th>\n",
       "      <td>17-18</td>\n",
       "      <td>102956</td>\n",
       "      <td>1</td>\n",
       "      <td>tooth_not_present</td>\n",
       "      <td>tooth_not_present</td>\n",
       "      <td>permanent_tooth_present</td>\n",
       "      <td>permanent_tooth_present</td>\n",
       "      <td>permanent_tooth_present</td>\n",
       "      <td>permanent_tooth_present</td>\n",
       "      <td>permanent_tooth_present</td>\n",
       "      <td>...</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35909 rows × 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     cohort  respondent_seq_num  dentition_status_code  \\\n",
       "0     11-12               62161                      1   \n",
       "1     11-12               62162                      1   \n",
       "2     11-12               62163                      1   \n",
       "3     11-12               62164                      1   \n",
       "4     11-12               62165                      1   \n",
       "...     ...                 ...                    ...   \n",
       "8361  17-18              102952                      1   \n",
       "8362  17-18              102953                      1   \n",
       "8363  17-18              102954                      1   \n",
       "8364  17-18              102955                      1   \n",
       "8365  17-18              102956                      1   \n",
       "\n",
       "                      ohx01tc                  ohx02tc  \\\n",
       "0           tooth_not_present  permanent_tooth_present   \n",
       "1           tooth_not_present        tooth_not_present   \n",
       "2           tooth_not_present  permanent_tooth_present   \n",
       "3           tooth_not_present  permanent_tooth_present   \n",
       "4           tooth_not_present  permanent_tooth_present   \n",
       "...                       ...                      ...   \n",
       "8361  permanent_tooth_present  permanent_tooth_present   \n",
       "8362  permanent_tooth_present  permanent_tooth_present   \n",
       "8363  permanent_tooth_present  permanent_tooth_present   \n",
       "8364        tooth_not_present  permanent_tooth_present   \n",
       "8365        tooth_not_present        tooth_not_present   \n",
       "\n",
       "                      ohx03tc                  ohx04tc  \\\n",
       "0     permanent_tooth_present  permanent_tooth_present   \n",
       "1           tooth_not_present    primary_tooth_present   \n",
       "2     permanent_tooth_present  permanent_tooth_present   \n",
       "3     permanent_tooth_present  permanent_tooth_present   \n",
       "4     permanent_tooth_present  permanent_tooth_present   \n",
       "...                       ...                      ...   \n",
       "8361  permanent_tooth_present  permanent_tooth_present   \n",
       "8362  permanent_tooth_present  permanent_tooth_present   \n",
       "8363  permanent_tooth_present  permanent_tooth_present   \n",
       "8364  permanent_tooth_present  permanent_tooth_present   \n",
       "8365  permanent_tooth_present  permanent_tooth_present   \n",
       "\n",
       "                      ohx05tc                  ohx06tc  \\\n",
       "0     permanent_tooth_present  permanent_tooth_present   \n",
       "1       primary_tooth_present    primary_tooth_present   \n",
       "2     permanent_tooth_present  permanent_tooth_present   \n",
       "3     permanent_tooth_present  permanent_tooth_present   \n",
       "4     permanent_tooth_present  permanent_tooth_present   \n",
       "...                       ...                      ...   \n",
       "8361  permanent_tooth_present  permanent_tooth_present   \n",
       "8362  permanent_tooth_present  permanent_tooth_present   \n",
       "8363  permanent_tooth_present  permanent_tooth_present   \n",
       "8364  permanent_tooth_present  permanent_tooth_present   \n",
       "8365  permanent_tooth_present  permanent_tooth_present   \n",
       "\n",
       "                      ohx07tc  ... ohx22ctc ohx23ctc ohx24ctc ohx25ctc  \\\n",
       "0     permanent_tooth_present  ...        S        S        S        S   \n",
       "1       primary_tooth_present  ...        D        D        D        D   \n",
       "2     permanent_tooth_present  ...        S        S        S        S   \n",
       "3     permanent_tooth_present  ...        S        S        S        S   \n",
       "4     permanent_tooth_present  ...        S        S        S        S   \n",
       "...                       ...  ...      ...      ...      ...      ...   \n",
       "8361  permanent_tooth_present  ...        S        S        S        S   \n",
       "8362  permanent_tooth_present  ...        S        S        S        S   \n",
       "8363  permanent_tooth_present  ...        S        S        S        S   \n",
       "8364  permanent_tooth_present  ...        S        S        S        S   \n",
       "8365  permanent_tooth_present  ...        S        S        S        S   \n",
       "\n",
       "     ohx26ctc ohx27ctc ohx28ctc ohx29ctc ohx30ctc ohx31ctc  \n",
       "0           S        S        S        S        Z        S  \n",
       "1           D        D        D        D        U        U  \n",
       "2           S        S        S        S        Y        S  \n",
       "3           S        S        S        S        Z        Z  \n",
       "4           S        S        S        S        S        S  \n",
       "...       ...      ...      ...      ...      ...      ...  \n",
       "8361        S        S        S        S        S        S  \n",
       "8362        S        S        S        S        S        Z  \n",
       "8363        S        S        S        F        S        S  \n",
       "8364        S        S        S        S        S        Z  \n",
       "8365        S        S        S        S        S        E  \n",
       "\n",
       "[35909 rows x 63 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohxden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "29b15f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the resulting data frame to a serialized “round-trip” format\n",
    "ohxden.to_pickle(\"./OHXDEN.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ba49a2",
   "metadata": {},
   "source": [
    "c. In your notebook, report the number of cases there are in the two datasets above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06f25307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[39156, 35909]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[demo.shape[0], ohxden.shape[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e9cfa0",
   "metadata": {},
   "source": [
    "Number of casess in the demographic dataset (the four cohorts spanning the years 2011-2018): 39156;\n",
    "<br>Number of cases in the oral health and dentition dataset (the four cohorts spanning the years 2011-2018): 35909."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
